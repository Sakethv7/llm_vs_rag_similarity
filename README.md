# Prompt Track Analysis

Automated analysis of user queries against a prompt library using LLM-based matching.

## ğŸš€ Quick Start

### Local Setup

1. **Clone and setup**
```bash
   git clone <your-repo>
   cd PROMPTTRACK_PUBLIC
   python -m venv .venv
   .venv\Scripts\activate  # Windows
   # source .venv/bin/activate  # Linux/Mac
   pip install -r requirements.txt
```

2. **Configure secrets**
```bash
   # Copy the example file
   copy .env.example .env  # Windows
   # cp .env.example .env  # Linux/Mac
   
   # Edit .env and add your OpenAI API key
   # OPENAI_API_KEY=sk-your-actual-key-here
```

3. **Run the pipeline**
```bash
   # Load datasets
   python scripts/load_datasets.py ^
     --queries-out data/queries.csv ^
     --prompts-out data/prompts.csv ^
     --limit 5000 ^
     --prompt-limit 200
   
   # Generate report
   python scripts/match_and_report.py ^
     --prompts data/prompts.csv ^
     --queries data/queries.csv ^
     --out reports/prompt_analysis.xlsx
```

## âš™ï¸ GitHub Actions Setup

### Setting up Secrets

1. Go to your GitHub repository
2. Click **Settings** â†’ **Secrets and variables** â†’ **Actions**
3. Click **New repository secret**
4. Add:
   - Name: `OPENAI_API_KEY`
   - Value: `sk-your-openai-key-here`

### Running the Workflow

The workflow runs automatically:
- **Daily at 2 AM UTC** (scheduled)
- **Manually** via the Actions tab

To run manually:
1. Go to **Actions** tab
2. Click **Generate Prompt Analysis Report**
3. Click **Run workflow**
4. (Optional) Adjust query/prompt limits
5. Click **Run workflow**

### Downloading Reports

After the workflow completes:
1. Go to the workflow run
2. Scroll to **Artifacts**
3. Download `prompt-analysis-report-XXX.xlsx`

## ğŸ“Š Output

The Excel report contains 4 sheets:
- **UserPromptUsage**: Every query with its matched prompt
- **PromptCoverage**: Which prompts are used and how often
- **UserAdoption**: Per-user adoption rates
- **UncoveredQueries**: Queries that didn't match any prompt

## ğŸ”§ Configuration

Edit `.env` for local runs:
```env
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini  # or gpt-4o for better accuracy
```

Edit `.github/workflows/generate_report.yml` to:
- Change schedule (cron expression)
- Adjust default limits
- Enable/disable auto-commit of reports

## ğŸ“ Project Structure
```
PROMPTTRACK_PUBLIC/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ generate_report.yml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ load_datasets.py
â”‚   â””â”€â”€ match_and_report.py
â”œâ”€â”€ data/              # Generated CSVs (gitignored)
â”œâ”€â”€ reports/           # Generated Excel files (gitignored)
â”œâ”€â”€ .env.example       # Template for secrets
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ’¡ Tips

- **Cost**: Using gpt-4o-mini costs ~$0.15 per 1M tokens (very cheap for this use case)
- **Speed**: Processing 5000 queries takes ~10-15 minutes
- **Rate Limits**: Adjust `--batch-size` if you hit rate limits
- **Large datasets**: Uncomment `data/*.csv` in `.gitignore` if CSVs are huge

## ğŸ”’ Security

- âœ… Never commit `.env` file
- âœ… Use GitHub Secrets for API keys
- âœ… `.gitignore` prevents accidental commits
- âœ… Use `python-dotenv` to load secrets safely